{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ffee433-f1da-48a3-9a87-8844ad7c326a",
   "metadata": {},
   "source": [
    "Build a structural feature matrix (n x m) from a people graph edge list.\n",
    "\n",
    "Input CSV: exactly two columns representing an undirected edge between person A and person B.\n",
    "- By default, the script assumes the CSV HAS a header row with column names.\n",
    "- Use --no-header if your CSV has no header. Then columns are referenced by zero-based indices.\n",
    "- Self-loops are removed; multiple edges are collapsed (unweighted, simple graph).\n",
    "\n",
    "Output CSV: rows = people (node IDs), columns = structural features.\n",
    "\n",
    "Fast features (always on):\n",
    "- degree\n",
    "- clustering_coef\n",
    "- triangle_count\n",
    "- k_core\n",
    "- avg_neighbor_degree\n",
    "- ego_edges\n",
    "- ego_density\n",
    "- pagerank\n",
    "\n",
    "Optional slower features (enable with --heavy):\n",
    "- eigenvector_centrality\n",
    "- betweenness_centrality\n",
    "- constraint (Burt)\n",
    "- effective_size (Burt)\n",
    "\n",
    "Usage:\n",
    "    python build_graph_features.py --input edges.csv --output features.csv \\\n",
    "        --src-col PersonA --dst-col PersonB\n",
    "    # or for no-header CSVs:\n",
    "    python build_graph_features.py --input edges.csv --output features.csv --no-header\n",
    "\n",
    "Notes:\n",
    "- For large graphs (e.g., > 1M edges), prefer running without --heavy first.\n",
    "- All features are structure-only (no attributes), computed on an undirected simple graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "927510a6-818e-4aa3-8f7c-17c8ef8fc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from typing import Tuple, Any\n",
    "import re\n",
    "from typing import Optional, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1afb36a4-dab3-4e0f-be03-09ab043cefbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove all characters from the input string except English alphabets (a-z, A-Z).\n",
    "    Consecutive non-alphabet characters are replaced with a single space.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text containing only English letters and spaces.\n",
    "    \"\"\"\n",
    "    # Keep only letters, replace everything else with space\n",
    "    cleaned = re.sub(r'[^A-Za-z]+', ' ', text)\n",
    "    # Strip extra whitespace\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3f248d9-f08c-46a9-af64-72c186a87080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_edges(path: str, has_header: bool, src_col: str, dst_col: str, sep: str) -> pd.DataFrame:\n",
    "    if has_header:\n",
    "        df = pd.read_csv(path, sep=sep)\n",
    "        if src_col is None or dst_col is None:\n",
    "            # Pick the first two columns by order\n",
    "            if df.shape[1] < 2:\n",
    "                raise ValueError(\"CSV must have at least two columns.\")\n",
    "            src_col = df.columns[0]\n",
    "            dst_col = df.columns[1]\n",
    "        # Coerce to string IDs (people names)\n",
    "        df[src_col] = df[src_col].astype(str).map(clean_text)\n",
    "        df[dst_col] = df[dst_col].astype(str).map(clean_text)\n",
    "        df = df[[src_col, dst_col]].rename(columns={src_col: \"src\", dst_col: \"dst\"})\n",
    "    else:\n",
    "        df = pd.read_csv(path, sep=sep, header=None)\n",
    "        if df.shape[1] < 2:\n",
    "            raise ValueError(\"CSV must have at least two columns.\")\n",
    "        # Use the first two columns\n",
    "        df = df[[0, 1]].rename(columns={0: \"src\", 1: \"dst\"})\n",
    "        df[\"src\"] = df[\"src\"].astype(str)\n",
    "        df[\"dst\"] = df[\"dst\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def build_graph(df: pd.DataFrame) -> nx.Graph:\n",
    "    # Undirected, simple graph\n",
    "    G = nx.Graph()\n",
    "    # Drop self loops\n",
    "    df = df[df[\"src\"] != df[\"dst\"]].copy()\n",
    "    # Add edges (NetworkX deduplicates multiple edges for Graph)\n",
    "    G.add_edges_from(df[[\"src\", \"dst\"]].itertuples(index=False, name=None))\n",
    "    return G\n",
    "\n",
    "def ego_stats(G: nx.Graph, node: Any) -> Tuple[int, float]:\n",
    "    d = G.degree(node)\n",
    "    if d < 2:\n",
    "        return 0, 0.0\n",
    "    nbrs = list(G.neighbors(node))\n",
    "    S = G.subgraph(nbrs)\n",
    "    e = S.number_of_edges()\n",
    "    # Density among neighbors only (not including the ego node)\n",
    "    dens = 2.0 * e / (d * (d - 1))\n",
    "    return e, dens\n",
    "\n",
    "def compute_features(G: nx.Graph, heavy: bool) -> pd.DataFrame:\n",
    "    nodes = list(G.nodes())\n",
    "    n = len(nodes)\n",
    "    if n == 0:\n",
    "        return pd.DataFrame(columns=[])\n",
    "\n",
    "    # Precompute frequently used dicts\n",
    "    deg = dict(G.degree())\n",
    "    clust = nx.clustering(G)  # local clustering coefficient\n",
    "    tri = nx.triangles(G)     # triangle count per node\n",
    "    core = nx.core_number(G)  # k-core index\n",
    "    andeg = nx.average_neighbor_degree(G)\n",
    "\n",
    "    # Ego stats\n",
    "    ego_e = {}\n",
    "    ego_d = {}\n",
    "    for v in nodes:\n",
    "        e, d = ego_stats(G, v)\n",
    "        ego_e[v] = e\n",
    "        ego_d[v] = d\n",
    "\n",
    "    # Pagerank (undirected graph is treated as symmetric directed)\n",
    "    pr = nx.pagerank(G, alpha=0.85, tol=1e-06)\n",
    "\n",
    "    # Initialize feature dict\n",
    "    feat = {\n",
    "        \"degree\": deg,\n",
    "        \"clustering_coef\": clust,\n",
    "        \"triangle_count\": tri,\n",
    "        \"k_core\": core,\n",
    "        \"avg_neighbor_degree\": andeg,\n",
    "        \"ego_edges\": ego_e,\n",
    "        \"ego_density\": ego_d,\n",
    "        \"pagerank\": pr,\n",
    "    }\n",
    "\n",
    "    if heavy:\n",
    "        # Eigenvector centrality\n",
    "        try:\n",
    "            ev = nx.eigenvector_centrality(G, max_iter=1000, tol=1e-06)\n",
    "            feat[\"eigenvector_centrality\"] = ev\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] eigenvector_centrality failed: {e}\")\n",
    "\n",
    "        # Betweenness centrality\n",
    "        try:\n",
    "            bc = nx.betweenness_centrality(G, normalized=True)\n",
    "            feat[\"betweenness_centrality\"] = bc\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] betweenness_centrality failed: {e}\")\n",
    "\n",
    "        # Structural holes metrics (Burt)\n",
    "        try:\n",
    "            eff = nx.effective_size(G)\n",
    "            feat[\"effective_size\"] = eff\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] effective_size failed: {e}\")\n",
    "        try:\n",
    "            con = nx.constraint(G)\n",
    "            feat[\"constraint\"] = con\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] constraint failed: {e}\")\n",
    "\n",
    "    # Assemble DataFrame with consistent node ordering\n",
    "    data = {k: pd.Series(v) for k, v in feat.items()}\n",
    "    X = pd.DataFrame(data, index=nodes).sort_index()\n",
    "    # Fill any missing values\n",
    "    X = X.reindex(sorted(nodes)).fillna(0.0)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "748facdc-ef17-427c-a8df-7d13707d4b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_matrix_from_df(\n",
    "    df: pd.DataFrame,\n",
    "    nodes: Optional[Sequence[str]] = None,\n",
    "    weighted: bool = False,\n",
    "    dtype=float,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build an undirected graph from a two-column edge DataFrame (src, dst)\n",
    "    and return its n x n adjacency matrix as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with columns [\"src\", \"dst\"] (strings).\n",
    "        nodes: Optional explicit node ordering. If None, uses all nodes\n",
    "               appearing in df, sorted.\n",
    "        weighted: If True, treat multiple edges as weights (counts).\n",
    "                  If False, simple 0/1 adjacency.\n",
    "        dtype: numpy/pandas dtype for the matrix values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: adjacency matrix with index/columns = node IDs.\n",
    "    \"\"\"\n",
    "    if not {\"src\", \"dst\"}.issubset(df.columns):\n",
    "        raise ValueError('df must have columns \"src\" and \"dst\"')\n",
    "\n",
    "    # Drop self-loops\n",
    "    edf = df[df[\"src\"] != df[\"dst\"]].copy()\n",
    "\n",
    "    if weighted:\n",
    "        # Count parallel edges between same pair\n",
    "        edf[\"w\"] = 1\n",
    "        wdf = (\n",
    "            edf.groupby([\"src\", \"dst\"], as_index=False)[\"w\"]\n",
    "            .sum()\n",
    "        )\n",
    "        G = nx.Graph()\n",
    "        G.add_weighted_edges_from(wdf[[\"src\", \"dst\", \"w\"]].itertuples(index=False, name=None))\n",
    "        weight_attr = \"weight\"\n",
    "    else:\n",
    "        # Simple graph (NetworkX deduplicates)\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edf[[\"src\", \"dst\"]].itertuples(index=False, name=None))\n",
    "        weight_attr = None\n",
    "\n",
    "    # Choose node ordering\n",
    "    if nodes is None:\n",
    "        nodes = sorted(G.nodes())\n",
    "    else:\n",
    "        # Ensure nodes exist in graph (include isolated if provided)\n",
    "        missing = set(nodes) - set(G.nodes())\n",
    "        if missing:\n",
    "            G.add_nodes_from(missing)\n",
    "\n",
    "    # Build adjacency as DataFrame\n",
    "    A = nx.to_numpy_array(G, nodelist=list(nodes), dtype=dtype, weight=weight_attr)\n",
    "    A_df = pd.DataFrame(A, index=nodes, columns=nodes)\n",
    "    return A_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "656e22d8-ef58-4b29-b105-e42df6716373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Nodes: 3661, Edges: 3753\n",
      "Wrote features to: .\n",
      "Columns: degree, clustering_coef, triangle_count, k_core, avg_neighbor_degree, ego_edges, ego_density, pagerank, eigenvector_centrality, betweenness_centrality, effective_size, constraint\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sep=\",\"\n",
    "no_header=True\n",
    "heavy=True\n",
    "\n",
    "df = read_edges(\"linked_In_endoresement_combined_csv.csv\", has_header=True, src_col=\"main_name\", dst_col=\"names\", sep=sep)\n",
    "G = build_graph(df)\n",
    "X = compute_features(G, heavy=heavy)\n",
    "\n",
    "# Save, with node names as first column\n",
    "X.index.name = \"person\"\n",
    "X.to_csv(\"feature_matrix.csv\")\n",
    "\n",
    "print(f\"Done. Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
    "print(f\"Wrote features to: .\")\n",
    "print(\"Columns:\", \", \".join(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88155870-eaf8-4c44-938f-fa4c0eb7c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"linkedin_feat.npy\",X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04859437-411d-4ae4-b7ab-afe5a8b9d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = adjacency_matrix_from_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df4e80e2-f710-4ebb-9034-33f675df9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"linkedin_adj.npy\",adj_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b88c68f-f0c0-4f4d-a720-f816f218ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = adj_matrix.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85241824-5935-47ab-8cc4-35a9bcf8dbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"src\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cac0e19c-7454-4bcc-b870-cdc56083df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_influencers = df[\"src\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "888f927d-99b9-4a87-99f2-e690afe70c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"linkedin_label.npy\",np.array([1 if node in all_influencers else 0 for node in all_nodes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8677f47-24c0-4d15-98e4-358420925f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kaveh)",
   "language": "python",
   "name": "kaveh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
