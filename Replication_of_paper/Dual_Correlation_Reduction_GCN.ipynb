{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07579184-a507-411a-ae2f-c87ab6efea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import opt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module, Parameter\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# from utils import *\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from munkres import Munkres\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec456c2-bb21-438f-b71a-557416920121",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda=True\n",
    "seed=3\n",
    "alpha_value=0.2\n",
    "lambda_value=10\n",
    "gamma_value=1e3\n",
    "lr=1e-4\n",
    "n_z=20\n",
    "epoch=400\n",
    "show_training_details=False\n",
    "    \n",
    "    \n",
    "    # AE structure parameter from DFCN\n",
    "ae_n_enc_1=128\n",
    "ae_n_enc_2=256\n",
    "ae_n_enc_3=512\n",
    "ae_n_dec_1=512\n",
    "ae_n_dec_2=256\n",
    "ae_n_dec_3=128\n",
    "    \n",
    "    # IGAE structure parameter from DFCN\n",
    "gae_n_enc_1=128\n",
    "gae_n_enc_2=256\n",
    "gae_n_enc_3=20\n",
    "gae_n_dec_1=20\n",
    "gae_n_dec_2=256\n",
    "gae_n_dec_3=128\n",
    "    \n",
    "    # clustering performance: acc, nmi, ari, f1\n",
    "acc=0\n",
    "nmi=0\n",
    "ari=0\n",
    "f1=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a89cddd-0c7e-4de8-839e-46871ebeb7ab",
   "metadata": {},
   "source": [
    "Here’s a line-by-line, shape-by-shape unpacking of that `GNNLayer` and what it’s doing.\n",
    "\n",
    "# What this layer is\n",
    "\n",
    "It’s a **GCN-style message passing layer**:\n",
    "\n",
    "1. linearly transform node features,\n",
    "2. aggregate transformed features from neighbors using the adjacency matrix,\n",
    "3. (optionally in this code path) aggregate once more to get a two-hop signal.\n",
    "\n",
    "It returns **one-hop output** and **two-hop output**.\n",
    "\n",
    "---\n",
    "\n",
    "# Constructor (`__init__`)\n",
    "\n",
    "```python\n",
    "self.in_features = in_features\n",
    "self.out_features = out_features\n",
    "if opt.args.name == \"dblp\":\n",
    "    self.act = nn.Tanh()\n",
    "    self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "else:\n",
    "    self.act = nn.Tanh()\n",
    "    self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "torch.nn.init.xavier_uniform_(self.weight)\n",
    "```\n",
    "\n",
    "* `in_features`, `out_features`: feature dimensions per node.\n",
    "* `self.act = nn.Tanh()`: activation function (note: ReLU is more common in GCNs; tanh is perfectly valid but can saturate).\n",
    "* `self.weight`: learnable projection matrix.\n",
    "\n",
    "  * **dblp case**: shape `(out_features, in_features)` — chosen to work with `F.linear`, which expects `(out_features, in_features)` and internally uses its transpose.\n",
    "  * **else**: shape `(in_features, out_features)` — chosen to work with `torch.mm` directly.\n",
    "* `xavier_uniform_`: good default for tanh/linear layers; helps keep activations in a reasonable range at init.\n",
    "\n",
    "### Why the odd “dblp” switch?\n",
    "\n",
    "`F.linear(x, W)` computes `x @ Wᵀ` with `W` stored as `(out, in)`.\n",
    "`torch.mm(x, W)` computes `x @ W` with `W` stored as `(in, out)`.\n",
    "The code keeps the stored shape aligned with the chosen multiply API so that the effective product is **always** `(N, in) → (N, out)`.\n",
    "\n",
    "---\n",
    "\n",
    "# Forward pass (`forward`)\n",
    "\n",
    "```python\n",
    "def forward(self, features, adj, active=False):\n",
    "    ...\n",
    "```\n",
    "\n",
    "**Expected shapes:**\n",
    "\n",
    "* `features`: `(N, in_features)` — dense float tensor (N = #nodes).\n",
    "* `adj`: `(N, N)` — **sparse** adjacency (usually symmetric, and typically pre-normalized outside this layer).\n",
    "* returns:\n",
    "\n",
    "  * `output`: `(N, out_features)` ≈ **A Z** (one-hop aggregation)\n",
    "  * `az`: `(N, out_features)` ≈ **A² Z** (two-hop aggregation)\n",
    "\n",
    "### 1) Linear transform (with optional activation)\n",
    "\n",
    "```python\n",
    "if active:\n",
    "    if opt.args.name == \"dblp\":\n",
    "        support = self.act(F.linear(features, self.weight))      # (N, out)\n",
    "    else:\n",
    "        support = self.act(torch.mm(features, self.weight))      # (N, out)\n",
    "else:\n",
    "    if opt.args.name == \"dblp\":\n",
    "        support = F.linear(features, self.weight)                # (N, out)\n",
    "    else:\n",
    "        support = torch.mm(features, self.weight)                # (N, out)\n",
    "```\n",
    "\n",
    "* `support` is the per-node **projected feature** matrix `Z` with shape `(N, out_features)`.\n",
    "* `active` flag: when `True`, apply `tanh` **before** neighborhood aggregation; when `False`, skip it (purely linear before aggregation). This lets the authors experiment with **pre-activation** vs **no pre-activation** variants.\n",
    "\n",
    "### 2) One-hop neighbor aggregation\n",
    "\n",
    "```python\n",
    "output = torch.spmm(adj, support)  # (N, N) @ (N, out) = (N, out)\n",
    "```\n",
    "\n",
    "* `torch.spmm` = sparse–dense matmul: efficient if `adj` is a `torch.sparse_*` tensor.\n",
    "* This computes **A Z**: for each node, sum (or weighted sum, if `adj` is normalized) of its neighbors’ transformed features.\n",
    "\n",
    "### 3) Two-hop aggregation\n",
    "\n",
    "```python\n",
    "az = torch.spmm(adj, output)       # (N, N) @ (N, out) = (N, out)\n",
    "return output, az\n",
    "```\n",
    "\n",
    "* Computes **A (A Z) = A² Z**: information aggregated from **two hops** away.\n",
    "* Naming hint: `az` ≈ “A times (A Z)”.\n",
    "* Returning both lets downstream code fuse 1-hop and 2-hop signals.\n",
    "\n",
    "---\n",
    "\n",
    "# Mathematical summary\n",
    "\n",
    "Let `X ∈ ℝ^{N×d_in}`, `W ∈ ℝ^{d_in×d_out}` (or `(d_out×d_in)` with `F.linear`), `A ∈ ℝ^{N×N}` (sparse). Define:\n",
    "\n",
    "* `Z = X W` or `tanh(X W)` depending on `active`.\n",
    "* `output = A Z` (one hop)\n",
    "* `az = A output = A² Z` (two hops)\n",
    "\n",
    "Note: In many GCNs, one uses a **normalized** adjacency `Â = D^{-1/2}(A+I)D^{-1/2}` with self-loops. This layer doesn’t normalize internally, so **whoever builds `adj` must handle normalization/self-loops** if desired.\n",
    "\n",
    "---\n",
    "\n",
    "# Practical notes & pitfalls\n",
    "\n",
    "* **Adjacency type**: `torch.spmm` requires a **sparse** adjacency. If you pass a dense tensor, use `torch.mm` instead.\n",
    "* **Normalization & self-loops**: If `adj` isn’t normalized / lacks self-loops, the scale and semantics of aggregation will differ (can cause exploding/vanishing or over-smoothing). Preprocess `adj` appropriately.\n",
    "* **Bias**: no bias term is used; if you need it, you’d add `bias` to the linear transform.\n",
    "* **Activation placement**: Only applied before aggregation when `active=True`. There’s no post-aggregation nonlinearity in this layer—some architectures add another `act` after `output`.\n",
    "* **Tanh choice**: Tanh can saturate; ReLU/LeakyReLU/ELU may train faster depending on the task.\n",
    "* **Return values**: Returning both `output` and `az` suggests the enclosing model (DFCN) may **fuse 1-hop and 2-hop** features for richer locality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164949e9-03d8-4d34-8d2c-e6f684ee96e6",
   "metadata": {},
   "source": [
    "Great — let’s break this `Readout` class down.\n",
    "\n",
    "---\n",
    "\n",
    "## High-level idea\n",
    "\n",
    "It’s a **readout function** that takes node embeddings `Z` (shape `(N, d)`) and compresses them into a **single graph-level (or cluster-level) embedding**.\n",
    "\n",
    "It does this by:\n",
    "\n",
    "1. Splitting the `N` nodes into `K` contiguous groups,\n",
    "2. Averaging the embeddings inside each group,\n",
    "3. Concatenating those group-level averages into a single vector.\n",
    "\n",
    "So it turns node-level features into a fixed-size representation of length `K * d`.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-step\n",
    "\n",
    "### Constructor\n",
    "\n",
    "```python\n",
    "def __init__(self, K):\n",
    "    self.K = K\n",
    "```\n",
    "\n",
    "* `K` = number of clusters / groups to partition the nodes into.\n",
    "* Controls the “resolution” of the readout: higher `K` = more groups preserved.\n",
    "\n",
    "---\n",
    "\n",
    "### Forward\n",
    "\n",
    "```python\n",
    "def forward(self, Z):\n",
    "    n_node = Z.shape[0]     # number of nodes N\n",
    "    step = n_node // self.K # how many nodes per group (integer division)\n",
    "```\n",
    "\n",
    "* `Z` is `(N, d)` (N nodes, each with d-dim embedding).\n",
    "* `step` is the group size, i.e. how many nodes per cluster.\n",
    "\n",
    "---\n",
    "\n",
    "### Grouping & averaging\n",
    "\n",
    "```python\n",
    "Z_tilde = []\n",
    "for i in range(0, n_node, step):\n",
    "    if n_node - i < 2 * step:\n",
    "        Z_tilde.append(torch.mean(Z[i:n_node], dim=0))\n",
    "        break\n",
    "    else:\n",
    "        Z_tilde.append(torch.mean(Z[i:i + step], dim=0))\n",
    "```\n",
    "\n",
    "* Loop slices `Z` into contiguous chunks of length `step`.\n",
    "* For each chunk:\n",
    "\n",
    "  * If we’re near the end and fewer than `2*step` nodes remain, it just averages *all remaining nodes* to avoid leaving a tiny last group.\n",
    "  * Otherwise, it averages exactly `step` nodes.\n",
    "* Each average is a vector of shape `(d,)`.\n",
    "\n",
    "So `Z_tilde` becomes a list of about `K` vectors, each `(d,)`.\n",
    "\n",
    "---\n",
    "\n",
    "### Concatenate\n",
    "\n",
    "```python\n",
    "Z_tilde = torch.cat(Z_tilde, dim=0)\n",
    "return Z_tilde.view(1, -1)\n",
    "```\n",
    "\n",
    "* Concatenates all group-average vectors into a long 1-D vector of length `K*d` (or close, depending on rounding).\n",
    "* `.view(1, -1)` reshapes it to `(1, K*d)` so the output is a **single row vector**.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "Say:\n",
    "\n",
    "* `N = 100` nodes,\n",
    "* embedding size `d = 64`,\n",
    "* `K = 10`.\n",
    "\n",
    "Then:\n",
    "\n",
    "* `step = 100 // 10 = 10`.\n",
    "* Splits nodes into \\~10 groups of size 10.\n",
    "* Averages each group → 10 vectors of shape `(64,)`.\n",
    "* Concatenates → `(640,)`.\n",
    "* Reshapes → `(1, 640)`.\n",
    "\n",
    "So the graph’s embedding is a single row vector of length `K * d`.\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "* **Why do this?** It creates a **fixed-length representation** regardless of the number of nodes in the input graph.\n",
    "* Instead of pooling all nodes into one vector (like global mean pooling), it keeps some *locality/partition structure* by using `K` groups.\n",
    "* Essentially: *\"structured graph readout via grouped averaging.\"*\n",
    "\n",
    "---\n",
    "\n",
    "⚡ Quick summary:\n",
    "This `Readout` takes all node embeddings `Z (N, d)`, splits the nodes into `K` contiguous chunks, averages each chunk, concatenates those averages, and outputs a single `(1, K*d)` embedding — a fixed-size vector summarizing the whole graph.\n",
    "\n",
    "---\n",
    "\n",
    "Do you want me to also show how this compares to standard **global mean pooling** in GNNs, and why this grouped strategy might be used in the DFCN model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0e120-5373-4b78-b36f-096a91a7eb32",
   "metadata": {},
   "source": [
    "Awesome—here’s a clear, shape-aware walkthrough of what this **Dual Correlation Reduction Network (DCRN)** is doing and how data flows through it.\n",
    "\n",
    "# What DCRN is (big picture)\n",
    "\n",
    "DCRN fuses two kinds of node representations—\n",
    "\n",
    "1. a **feature-only Autoencoder (AE)** path, and\n",
    "2. a **graph-structure-aware Improved Graph Autoencoder (IGAE)** path—\n",
    "   then **reduces correlations** and **refines** them through graph-based smoothing and similarity propagation. It also keeps **cluster centers** and computes **soft assignments** (like DEC) for clustering. A readout block summarizes node embeddings into cluster-level vectors.\n",
    "\n",
    "---\n",
    "\n",
    "# Modules & learnable parameters\n",
    "\n",
    "## 1) Autoencoder (AE)\n",
    "\n",
    "```python\n",
    "self.ae = AE(..., n_input=opt.args.n_input, n_z=opt.args.n_z)\n",
    "```\n",
    "\n",
    "* Encodes each node’s feature vector (dim `n_input`) into a latent vector of dim `n_z`, then can decode back to features.\n",
    "* Shape-wise: if `X` is `(N, n_input)`, `AE.encoder(X)` → `Z_ae` of shape `(N, n_z)`; `AE.decoder(Z)` → `X_hat` `(N, n_input)`.\n",
    "\n",
    "## 2) Improved Graph Autoencoder (IGAE)\n",
    "\n",
    "```python\n",
    "self.gae = IGAE(..., n_input=opt.args.n_input)\n",
    "```\n",
    "\n",
    "* Encodes node features **with adjacency** to produce a **graph-aware** embedding of fixed size **20** (per the args).\n",
    "* The encoder returns:\n",
    "\n",
    "  * `Z_igae` `(N, 20)` — node embeddings,\n",
    "  * `A_igae` `(N, N)` — a learned/denoised adjacency (or similarity) from IGAE,\n",
    "  * `AZ` — a list of propagated embeddings at intermediate layers (A·Z, A²·Z, …),\n",
    "  * `Z_list` — a list of intermediate embeddings before propagation.\n",
    "* The decoder reconstructs both node embeddings and graph structure.\n",
    "\n",
    "## 3) Fusion parameters (from DFCN)\n",
    "\n",
    "```python\n",
    "self.a = Parameter( ... shape (n_node, 20) filled with 0.5 ... )\n",
    "self.b = Parameter( ... shape (n_node, 20) filled with 0.5 ... )\n",
    "self.alpha = Parameter(torch.zeros(1))\n",
    "```\n",
    "\n",
    "* **`a` and `b`**: per-node, per-dimension gates that **blend** AE vs IGAE embeddings:\n",
    "\n",
    "  * `Z_i = a ⊙ Z_ae + b ⊙ Z_igae`.\n",
    "  * Shapes must match: this implies **`opt.args.n_z == 20`** so that `Z_ae` and `Z_igae` have the same last dimension (20).\n",
    "* **`alpha`**: scalar mixing weight to combine a graph-similarity propagated embedding with a directly smoothed one (see the fusion block below).\n",
    "\n",
    "## 4) Cluster centers & Readout\n",
    "\n",
    "```python\n",
    "self.cluster_centers = Parameter(torch.Tensor(n_clusters, 20))\n",
    "self.R = Readout(K=n_clusters)\n",
    "```\n",
    "\n",
    "* `cluster_centers`: the learnable centroids (one per cluster) in the **20-dim** space used for **soft assignments**.\n",
    "* `Readout(K)`: takes node embeddings `(N, d)` and returns a **single** row vector `(1, K*d)` by splitting nodes into `K` groups and averaging per group (you already examined this).\n",
    "\n",
    "---\n",
    "\n",
    "# Soft assignment (DEC-style)\n",
    "\n",
    "```python\n",
    "def q_distribute(self, Z, Z_ae, Z_igae):\n",
    "    q = 1 / (1 + sum((Z - c_k)^2))       # Student-t kernel vs each center\n",
    "    q = q / row_sums(q)                   # normalize per node\n",
    "    # same for q_ae and q_igae\n",
    "    return [q, q_ae, q_igae]\n",
    "```\n",
    "\n",
    "* For each node and each cluster center, compute a **Student-t** similarity (no explicit degree-of-freedom term; effectively ν=1).\n",
    "* Row-normalize to get probabilities.\n",
    "* Do it for the **fusion** embedding `Z`, and **also** for the individual streams `Z_ae` and `Z_igae`.\n",
    "* Output: three `(N, n_clusters)` matrices.\n",
    "\n",
    "---\n",
    "\n",
    "# Forward pass (data flow)\n",
    "\n",
    "Inputs:\n",
    "\n",
    "* `X_tilde1, X_tilde2`: two **views** of node features, each `(N, n_input)` (e.g., two augmentations or two modalities).\n",
    "* `Am, Ad`: two adjacency matrices `(N, N)` (e.g., different graphs/views: “m” vs “d”). Typically sparse or normalized.\n",
    "\n",
    "### 1) Encode each view with AE\n",
    "\n",
    "```python\n",
    "Z_ae1 = AE.encoder(X_tilde1)   # (N, 20)\n",
    "Z_ae2 = AE.encoder(X_tilde2)   # (N, 20)\n",
    "```\n",
    "\n",
    "> assumes `n_z == 20` so it can fuse with IGAE downstream.\n",
    "\n",
    "### 2) Encode each view with IGAE\n",
    "\n",
    "```python\n",
    "Z_igae1, A_igae1, AZ_1, Z_1 = IGAE.encoder(X_tilde1, Am)  # (N,20), (N,N), [layerwise], [layerwise]\n",
    "Z_igae2, A_igae2, AZ_2, Z_2 = IGAE.encoder(X_tilde2, Ad)\n",
    "```\n",
    "\n",
    "* You get graph-aware embeddings for each view and ancillary layerwise signals.\n",
    "\n",
    "### 3) Cluster-level readouts (per view, per stream)\n",
    "\n",
    "```python\n",
    "Z_tilde_ae1   = R(Z_ae1)    # (1, n_clusters * 20)\n",
    "Z_tilde_ae2   = R(Z_ae2)\n",
    "Z_tilde_igae1 = R(Z_igae1)\n",
    "Z_tilde_igae2 = R(Z_igae2)\n",
    "```\n",
    "\n",
    "* These summarize node embeddings into fixed-size vectors per graph (or per batch item).\n",
    "\n",
    "### 4) Average the two views within each stream\n",
    "\n",
    "```python\n",
    "Z_ae   = (Z_ae1   + Z_ae2) / 2     # (N, 20)\n",
    "Z_igae = (Z_igae1 + Z_igae2) / 2   # (N, 20)\n",
    "```\n",
    "\n",
    "### 5) **Fusion block** (DFCN style)\n",
    "\n",
    "```python\n",
    "Z_i = a ⊙ Z_ae + b ⊙ Z_igae        # (N, 20)  elementwise blend, per-node, per-dim gates\n",
    "Z_l = Am · Z_i                     # (N, 20)  one-hop smoothing by Am (graph propagation)\n",
    "S   = softmax( Z_l · Z_lᵀ, dim=1 ) # (N, N)   node–node similarity with row-wise softmax\n",
    "Z_g = S · Z_l                      # (N, 20)  propagate via learned similarities\n",
    "Z   = alpha * Z_g + Z_l            # (N, 20)  residual mix (learned scalar alpha)\n",
    "```\n",
    "\n",
    "Intuition:\n",
    "\n",
    "* `Z_i`: blend feature-only vs graph-aware.\n",
    "* `Z_l`: smooth over neighbors via `Am`.\n",
    "* `S`: build a **data-driven affinity** from inner products of smoothed embeddings, normalize rows.\n",
    "* `Z_g`: propagate through this learned affinity.\n",
    "* `Z`: residual mixing between structural smoothing (`Z_l`) and similarity propagation (`Z_g`) controlled by `alpha`.\n",
    "\n",
    "### 6) Reconstructions\n",
    "\n",
    "**AE decoding (feature reconstruction)**\n",
    "\n",
    "```python\n",
    "X_hat = AE.decoder(Z)              # (N, n_input)\n",
    "```\n",
    "\n",
    "* Reconstructs node features from fused embedding → AE loss term.\n",
    "\n",
    "**IGAE decoding (graph/embedding reconstruction)**\n",
    "\n",
    "```python\n",
    "Z_hat, Z_adj_hat, AZ_de, Z_de = IGAE.decoder(Z, Am)\n",
    "sim = (A_igae1 + A_igae2) / 2      # averaged structure from both views\n",
    "A_hat = sim + Z_adj_hat            # combine encoder-learned sim with decoder’s adj recon\n",
    "```\n",
    "\n",
    "* `Z_adj_hat` is a reconstructed adjacency (or similarity) from the decoder path.\n",
    "* `A_hat` mixes the encoder-side learned structure (`sim`) and decoder reconstruction.\n",
    "\n",
    "### 7) Package stream outputs\n",
    "\n",
    "```python\n",
    "Z_ae_all  = [Z_ae1, Z_ae2, Z_tilde_ae1,  Z_tilde_ae2]   # node-level & readout for AE stream\n",
    "Z_gae_all = [Z_igae1, Z_igae2, Z_tilde_igae1, Z_tilde_igae2]\n",
    "```\n",
    "\n",
    "### 8) Soft assignments (for clustering)\n",
    "\n",
    "```python\n",
    "Q = q_distribute(Z, Z_ae, Z_igae)  # [q, q_ae, q_igae], each (N, n_clusters)\n",
    "```\n",
    "\n",
    "### 9) Layerwise propagated signals for supervision/aux\n",
    "\n",
    "```python\n",
    "AZ_en = [(AZ_1[i] + AZ_2[i]) / 2 for i in range(len(AZ_1))]  # encoder-level propagated\n",
    "Z_en  = [(Z_1[i]  + Z_2[i])  / 2 for i in range(len(Z_1))]   # encoder-level embeddings\n",
    "AZ_all = [AZ_en, AZ_de]  # [encoder side list, decoder side list]\n",
    "Z_all  = [Z_en,  Z_de]   # same for embeddings\n",
    "```\n",
    "\n",
    "* These collections enable multi-level losses (e.g., consistency between encoder/decoder layers, correlation penalties, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "# Returns (in order)\n",
    "\n",
    "```python\n",
    "return (\n",
    "  X_hat,          # (N, n_input): AE reconstruction from fused Z\n",
    "  Z_hat,          # (N, 20):     IGAE-decoder embedding reconstruction\n",
    "  A_hat,          # (N, N):      combined reconstructed adjacency/similarity\n",
    "  sim,            # (N, N):      averaged encoder-learned structure from both views\n",
    "  Z_ae_all,       # [Z_ae1, Z_ae2, readout_ae1, readout_ae2]\n",
    "  Z_gae_all,      # [Z_igae1, Z_igae2, readout_igae1, readout_igae2]\n",
    "  Q,              # [q, q_ae, q_igae] soft assignments (N, n_clusters each)\n",
    "  Z,              # (N, 20):     final fused node embeddings\n",
    "  AZ_all,         # [[AZ_en_layers...], AZ_de_layers...] propagated embeddings\n",
    "  Z_all           # [[Z_en_layers...],  Z_de_layers... ]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Why these pieces exist (training/use)\n",
    "\n",
    "* **AE loss** on `X_hat` encourages preserving feature information.\n",
    "* **IGAE losses** on `Z_hat`, `A_hat` encourage structural fidelity and robust graph modeling.\n",
    "* **Fusion & correlation reduction**: elementwise gates (`a`, `b`), smoothing (`Am`), and similarity propagation (`S`) reduce redundancy and align views.\n",
    "* **Clustering**: `cluster_centers` + `Q` enable DEC-like training (KL to a sharpened target `P`) to produce clusterable embeddings.\n",
    "* **Readouts** provide graph-level/cluster-level summaries that can be used for auxiliary objectives or downstream tasks.\n",
    "\n",
    "---\n",
    "\n",
    "# Key assumptions & gotchas\n",
    "\n",
    "* **Dimensionality match**: Because `a`/`b` are `(n_node, 20)`, you need `opt.args.n_z == 20` so `Z_ae` and `Z_igae` can be blended elementwise.\n",
    "* **Adjacency normalization**: Good practice to pass **normalized** `Am`, `Ad` (e.g., $\\hat A$) for stable propagation.\n",
    "* **`alpha` init**: starts at 0 → the model initially trusts `Z_l` more than `Z_g`, and can learn to upweight `Z_g`.\n",
    "* **Two views**: The design assumes two complementary inputs (feature views and/or graph views) that benefit from co-regularization.\n",
    "\n",
    "If you want, I can sketch typical **loss terms** this architecture would pair with (AE recon, IGAE recon, KL to target distribution for clustering, correlation penalties between views, etc.) and how you’d wire them up to train end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eb88ff5-c8ae-4ffd-ac35-109ebc11bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE encoder from DFCN\n",
    "class AE_encoder(nn.Module):\n",
    "    def __init__(self, ae_n_enc_1, ae_n_enc_2, ae_n_enc_3, n_input, n_z):\n",
    "        super(AE_encoder, self).__init__()\n",
    "        self.enc_1 = Linear(n_input, ae_n_enc_1)\n",
    "        self.enc_2 = Linear(ae_n_enc_1, ae_n_enc_2)\n",
    "        self.enc_3 = Linear(ae_n_enc_2, ae_n_enc_3)\n",
    "        self.z_layer = Linear(ae_n_enc_3, n_z)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.act(self.enc_1(x))\n",
    "        z = self.act(self.enc_2(z))\n",
    "        z = self.act(self.enc_3(z))\n",
    "        z_ae = self.z_layer(z)\n",
    "        return z_ae\n",
    "\n",
    "\n",
    "# AE decoder from DFCN\n",
    "class AE_decoder(nn.Module):\n",
    "    def __init__(self, ae_n_dec_1, ae_n_dec_2, ae_n_dec_3, n_input, n_z):\n",
    "        super(AE_decoder, self).__init__()\n",
    "\n",
    "        self.dec_1 = Linear(n_z, ae_n_dec_1)\n",
    "        self.dec_2 = Linear(ae_n_dec_1, ae_n_dec_2)\n",
    "        self.dec_3 = Linear(ae_n_dec_2, ae_n_dec_3)\n",
    "        self.x_bar_layer = Linear(ae_n_dec_3, n_input)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, z_ae):\n",
    "        z = self.act(self.dec_1(z_ae))\n",
    "        z = self.act(self.dec_2(z))\n",
    "        z = self.act(self.dec_3(z))\n",
    "        x_hat = self.x_bar_layer(z)\n",
    "        return x_hat\n",
    "\n",
    "\n",
    "# Auto Encoder from DFCN\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, ae_n_enc_1, ae_n_enc_2, ae_n_enc_3, ae_n_dec_1, ae_n_dec_2, ae_n_dec_3, n_input, n_z):\n",
    "        super(AE, self).__init__()\n",
    "\n",
    "        self.encoder = AE_encoder(\n",
    "            ae_n_enc_1=ae_n_enc_1,\n",
    "            ae_n_enc_2=ae_n_enc_2,\n",
    "            ae_n_enc_3=ae_n_enc_3,\n",
    "            n_input=n_input,\n",
    "            n_z=n_z)\n",
    "\n",
    "        self.decoder = AE_decoder(\n",
    "            ae_n_dec_1=ae_n_dec_1,\n",
    "            ae_n_dec_2=ae_n_dec_2,\n",
    "            ae_n_dec_3=ae_n_dec_3,\n",
    "            n_input=n_input,\n",
    "            n_z=n_z)\n",
    "\n",
    "\n",
    "# GNNLayer from DFCN\n",
    "class GNNLayer(Module):\n",
    "    def __init__(self, in_features, out_features,dataset_name:str=\"amac\"):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dataset_name = dataset_name\n",
    "        if self.dataset_name == \"dblp\":\n",
    "            self.act = nn.Tanh()\n",
    "            self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        else:\n",
    "            self.act = nn.Tanh()\n",
    "            self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, features, adj, active=False):\n",
    "        if active:\n",
    "            if self.dataset_name == \"dblp\":\n",
    "                support = self.act(F.linear(features, self.weight))\n",
    "            else:\n",
    "                support = self.act(torch.mm(features, self.weight))\n",
    "        else:\n",
    "            if self.dataset_name == \"dblp\":\n",
    "                support = F.linear(features, self.weight)\n",
    "            else:\n",
    "                support = torch.mm(features, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        az = torch.spmm(adj, output)\n",
    "        return output, az\n",
    "\n",
    "\n",
    "# IGAE encoder from DFCN\n",
    "class IGAE_encoder(nn.Module):\n",
    "    def __init__(self, gae_n_enc_1, gae_n_enc_2, gae_n_enc_3, n_input):\n",
    "        super(IGAE_encoder, self).__init__()\n",
    "        self.gnn_1 = GNNLayer(n_input, gae_n_enc_1)\n",
    "        self.gnn_2 = GNNLayer(gae_n_enc_1, gae_n_enc_2)\n",
    "        self.gnn_3 = GNNLayer(gae_n_enc_2, gae_n_enc_3)\n",
    "        self.s = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        z_1, az_1 = self.gnn_1(x, adj, active=True)\n",
    "        z_2, az_2 = self.gnn_2(z_1, adj, active=True)\n",
    "        z_igae, az_3 = self.gnn_3(z_2, adj, active=False)\n",
    "        z_igae_adj = self.s(torch.mm(z_igae, z_igae.t()))\n",
    "        return z_igae, z_igae_adj, [az_1, az_2, az_3], [z_1, z_2, z_igae]\n",
    "\n",
    "\n",
    "# IGAE decoder from DFCN\n",
    "class IGAE_decoder(nn.Module):\n",
    "    def __init__(self, gae_n_dec_1, gae_n_dec_2, gae_n_dec_3, n_input):\n",
    "        super(IGAE_decoder, self).__init__()\n",
    "        self.gnn_4 = GNNLayer(gae_n_dec_1, gae_n_dec_2)\n",
    "        self.gnn_5 = GNNLayer(gae_n_dec_2, gae_n_dec_3)\n",
    "        self.gnn_6 = GNNLayer(gae_n_dec_3, n_input)\n",
    "        self.s = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z_igae, adj):\n",
    "        z_1, az_1 = self.gnn_4(z_igae, adj, active=True)\n",
    "        z_2, az_2 = self.gnn_5(z_1, adj, active=True)\n",
    "        z_hat, az_3 = self.gnn_6(z_2, adj, active=True)\n",
    "        z_hat_adj = self.s(torch.mm(z_hat, z_hat.t()))\n",
    "        return z_hat, z_hat_adj, [az_1, az_2, az_3], [z_1, z_2, z_hat]\n",
    "\n",
    "\n",
    "# Improved Graph Auto Encoder from DFCN\n",
    "class IGAE(nn.Module):\n",
    "    def __init__(self, gae_n_enc_1, gae_n_enc_2, gae_n_enc_3, gae_n_dec_1, gae_n_dec_2, gae_n_dec_3, n_input):\n",
    "        super(IGAE, self).__init__()\n",
    "        # IGAE encoder\n",
    "        self.encoder = IGAE_encoder(\n",
    "            gae_n_enc_1=gae_n_enc_1,\n",
    "            gae_n_enc_2=gae_n_enc_2,\n",
    "            gae_n_enc_3=gae_n_enc_3,\n",
    "            n_input=n_input)\n",
    "\n",
    "        # IGAE decoder\n",
    "        self.decoder = IGAE_decoder(\n",
    "            gae_n_dec_1=gae_n_dec_1,\n",
    "            gae_n_dec_2=gae_n_dec_2,\n",
    "            gae_n_dec_3=gae_n_dec_3,\n",
    "            n_input=n_input)\n",
    "\n",
    "\n",
    "# readout function\n",
    "class Readout(nn.Module):\n",
    "    def __init__(self, K):\n",
    "        super(Readout, self).__init__()\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, Z):\n",
    "        # calculate cluster-level embedding\n",
    "        Z_tilde = []\n",
    "\n",
    "        # step1: split the nodes into K groups\n",
    "        # step2: average the node embedding in each group\n",
    "        n_node = Z.shape[0]\n",
    "        step = n_node // self.K\n",
    "        for i in range(0, n_node, step):\n",
    "            if n_node - i < 2 * step:\n",
    "                Z_tilde.append(torch.mean(Z[i:n_node], dim=0))\n",
    "                break\n",
    "            else:\n",
    "                Z_tilde.append(torch.mean(Z[i:i + step], dim=0))\n",
    "\n",
    "        # the cluster-level embedding\n",
    "        Z_tilde = torch.cat(Z_tilde, dim=0)\n",
    "        return Z_tilde.view(1, -1)\n",
    "\n",
    "\n",
    "# Dual Correlation Reduction Network\n",
    "class DCRN(nn.Module):\n",
    "    def __init__(self, n_node=None):\n",
    "        super(DCRN, self).__init__()\n",
    "\n",
    "        # Auto Encoder\n",
    "        self.ae = AE(\n",
    "            ae_n_enc_1=ae_n_enc_1,\n",
    "            ae_n_enc_2=ae_n_enc_2,\n",
    "            ae_n_enc_3=ae_n_enc_3,\n",
    "            ae_n_dec_1=ae_n_dec_1,\n",
    "            ae_n_dec_2=ae_n_dec_2,\n",
    "            ae_n_dec_3=ae_n_dec_3,\n",
    "            n_input=n_input,\n",
    "            n_z=n_z)\n",
    "\n",
    "        # Improved Graph Auto Encoder From DFCN\n",
    "        self.gae = IGAE(\n",
    "            gae_n_enc_1=gae_n_enc_1,\n",
    "            gae_n_enc_2=gae_n_enc_2,\n",
    "            gae_n_enc_3=gae_n_enc_3,\n",
    "            gae_n_dec_1=gae_n_dec_1,\n",
    "            gae_n_dec_2=gae_n_dec_2,\n",
    "            gae_n_dec_3=gae_n_dec_3,\n",
    "            n_input=n_input)\n",
    "\n",
    "        # fusion parameter from DFCN\n",
    "        self.a = Parameter(nn.init.constant_(torch.zeros(n_node, n_z), 0.5), requires_grad=True)\n",
    "        self.b = Parameter(nn.init.constant_(torch.zeros(n_node, n_z), 0.5), requires_grad=True)\n",
    "        self.alpha = Parameter(torch.zeros(1))\n",
    "\n",
    "        # cluster layer (clustering assignment matrix)\n",
    "        self.cluster_centers = Parameter(torch.Tensor(n_clusters, n_z), requires_grad=True)\n",
    "\n",
    "        # readout function\n",
    "        self.R = Readout(K=n_clusters)\n",
    "\n",
    "    # calculate the soft assignment distribution Q\n",
    "    def q_distribute(self, Z, Z_ae, Z_igae):\n",
    "        \"\"\"\n",
    "        calculate the soft assignment distribution based on the embedding and the cluster centers\n",
    "        Args:\n",
    "            Z: fusion node embedding\n",
    "            Z_ae: node embedding encoded by AE\n",
    "            Z_igae: node embedding encoded by IGAE\n",
    "        Returns:\n",
    "            the soft assignment distribution Q\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + torch.sum(torch.pow(Z.unsqueeze(1) - self.cluster_centers, 2), 2))\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "\n",
    "        q_ae = 1.0 / (1.0 + torch.sum(torch.pow(Z_ae.unsqueeze(1) - self.cluster_centers, 2), 2))\n",
    "        q_ae = (q_ae.t() / torch.sum(q_ae, 1)).t()\n",
    "\n",
    "        q_igae = 1.0 / (1.0 + torch.sum(torch.pow(Z_igae.unsqueeze(1) - self.cluster_centers, 2), 2))\n",
    "        q_igae = (q_igae.t() / torch.sum(q_igae, 1)).t()\n",
    "\n",
    "        return [q, q_ae, q_igae]\n",
    "\n",
    "    def forward(self, X_tilde1, Am, X_tilde2, Ad):\n",
    "        # node embedding encoded by AE\n",
    "        Z_ae1 = self.ae.encoder(X_tilde1)\n",
    "        Z_ae2 = self.ae.encoder(X_tilde2)\n",
    "\n",
    "        # node embedding encoded by IGAE\n",
    "        Z_igae1, A_igae1, AZ_1, Z_1 = self.gae.encoder(X_tilde1, Am)\n",
    "        Z_igae2, A_igae2, AZ_2, Z_2 = self.gae.encoder(X_tilde2, Ad)\n",
    "\n",
    "        # cluster-level embedding calculated by readout function\n",
    "        Z_tilde_ae1 = self.R(Z_ae1)\n",
    "        Z_tilde_ae2 = self.R(Z_ae2)\n",
    "        Z_tilde_igae1 = self.R(Z_igae1)\n",
    "        Z_tilde_igae2 = self.R(Z_igae2)\n",
    "\n",
    "        # linear combination of view 1 and view 2\n",
    "        Z_ae = (Z_ae1 + Z_ae2) / 2\n",
    "        Z_igae = (Z_igae1 + Z_igae2) / 2\n",
    "\n",
    "        # node embedding fusion from DFCN\n",
    "        Z_i = self.a * Z_ae + self.b * Z_igae\n",
    "        Z_l = torch.spmm(Am, Z_i)\n",
    "        S = torch.mm(Z_l, Z_l.t())\n",
    "        S = F.softmax(S, dim=1)\n",
    "        Z_g = torch.mm(S, Z_l)\n",
    "        Z = self.alpha * Z_g + Z_l\n",
    "\n",
    "        # AE decoding\n",
    "        X_hat = self.ae.decoder(Z)\n",
    "\n",
    "        # IGAE decoding\n",
    "        Z_hat, Z_adj_hat, AZ_de, Z_de = self.gae.decoder(Z, Am)\n",
    "        sim = (A_igae1 + A_igae2) / 2\n",
    "        A_hat = sim + Z_adj_hat\n",
    "\n",
    "        # node embedding and cluster-level embedding\n",
    "        Z_ae_all = [Z_ae1, Z_ae2, Z_tilde_ae1, Z_tilde_ae2]\n",
    "        Z_gae_all = [Z_igae1, Z_igae2, Z_tilde_igae1, Z_tilde_igae2]\n",
    "\n",
    "        # the soft assignment distribution Q\n",
    "        Q = self.q_distribute(Z, Z_ae, Z_igae)\n",
    "\n",
    "        # propagated embedding AZ_all and embedding Z_all\n",
    "        AZ_en = []\n",
    "        Z_en = []\n",
    "        for i in range(len(AZ_1)):\n",
    "            AZ_en.append((AZ_1[i]+AZ_2[i])/2)\n",
    "            Z_en.append((Z_1[i]+Z_2[i])/2)\n",
    "        AZ_all = [AZ_en, AZ_de]\n",
    "        Z_all = [Z_en, Z_de]\n",
    "\n",
    "        return X_hat, Z_hat, A_hat, sim, Z_ae_all, Z_gae_all, Q, Z, AZ_all, Z_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48924cfc-b2d1-45fa-9961-2db9d1230d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 5]\n",
      " [3 4 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.array([[1, 2], [3, 4]])\n",
    "vector = np.array([5, 6])\n",
    "\n",
    "# Stack the vector as a new column\n",
    "result = np.column_stack((matrix, vector))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f83aab-5c58-473c-97ce-f19f5e8be1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'amac'\n",
    "n_clusters = 10\n",
    "n_input = 20\n",
    "alpha_value = 0.2\n",
    "lambda_value = 10\n",
    "gamma_value = 1e3\n",
    "lr = 1e-3\n",
    "\n",
    "# if opt.args.name == 'acm':\n",
    "#         opt.args.n_clusters = 3\n",
    "#         opt.args.n_input = 100\n",
    "#         opt.args.alpha_value = 0.2\n",
    "#         opt.args.lambda_value = 10\n",
    "#         opt.args.gamma_value = 1e3\n",
    "#         opt.args.lr = 5e-5\n",
    "\n",
    "#     elif opt.args.name == 'dblp':\n",
    "#         opt.args.n_clusters = 4\n",
    "#         opt.args.n_input = 50\n",
    "#         opt.args.alpha_value = 0.2\n",
    "#         opt.args.lambda_value = 10\n",
    "#         opt.args.gamma_value = 1e3\n",
    "#         opt.args.lr = 1e-4\n",
    "\n",
    "#     elif opt.args.name == 'cite':\n",
    "#         opt.args.n_clusters = 6\n",
    "#         opt.args.n_input = 100\n",
    "#         opt.args.alpha_value = 0.2\n",
    "#         opt.args.lambda_value = 10\n",
    "#         opt.args.gamma_value = 1e3\n",
    "#         opt.args.lr = 1e-5\n",
    "\n",
    "#     elif opt.args.name == 'amap':\n",
    "#         opt.args.n_clusters = 8\n",
    "#         opt.args.n_input = 100\n",
    "#         opt.args.alpha_value = 0.2\n",
    "#         opt.args.lambda_value = 10\n",
    "#         opt.args.gamma_value = 1e3\n",
    "#         opt.args.lr = 1e-3\n",
    "\n",
    "#     elif opt.args.name == 'wiki':\n",
    "#         opt.args.n_clusters = 17\n",
    "#         opt.args.n_input = 100\n",
    "#         opt.args.alpha_value = 0.2\n",
    "#         opt.args.lambda_value = 10\n",
    "#         opt.args.gamma_value = 1e3\n",
    "#         opt.args.lr = 1e-3\n",
    "\n",
    "    # elif opt.args.name == 'amac':\n",
    "    #     opt.args.n_clusters = 10\n",
    "    #     opt.args.n_input = 100\n",
    "    #     opt.args.alpha_value = 0.2\n",
    "    #     opt.args.lambda_value = 10\n",
    "    #     opt.args.gamma_value = 1e3\n",
    "    #     opt.args.lr = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    \"\"\"\n",
    "    setup random seed to fix the result\n",
    "    Args:\n",
    "        seed: random seed\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "setup_seed(32)\n",
    "\n",
    "def numpy_to_torch(a, sparse=False):\n",
    "    \"\"\"\n",
    "    numpy array to torch tensor\n",
    "    :param a: the numpy array\n",
    "    :param sparse: is sparse tensor or not\n",
    "    :return: torch tensor\n",
    "    \"\"\"\n",
    "    if sparse:\n",
    "        a = torch.sparse.Tensor(a)\n",
    "        a = a.to_sparse()\n",
    "    else:\n",
    "        a = torch.FloatTensor(a)\n",
    "    return a\n",
    "\n",
    "\n",
    "def torch_to_numpy(t):\n",
    "    \"\"\"\n",
    "    torch tensor to numpy array\n",
    "    :param t: the torch tensor\n",
    "    :return: numpy array\n",
    "    \"\"\"\n",
    "    return t.numpy()\n",
    "\n",
    "\n",
    "def load_graph_data(dataset_name, show_details=True):\n",
    "    \"\"\"\n",
    "    load graph data\n",
    "    :param dataset_name: the name of the dataset\n",
    "    :param show_details: if show the details of dataset\n",
    "    - dataset name\n",
    "    - features' shape\n",
    "    - labels' shape\n",
    "    - adj shape\n",
    "    - edge num\n",
    "    - category num\n",
    "    - category distribution\n",
    "    :return: the features, labels and adj\n",
    "    \"\"\"\n",
    "    load_path = \"./DCRN-main/dataset/\" + dataset_name + \"/\" + dataset_name\n",
    "    feat = np.load(load_path+\"_feat.npy\", allow_pickle=True)\n",
    "    label = np.load(load_path+\"_label.npy\", allow_pickle=True)\n",
    "    adj = np.load(load_path+\"_adj.npy\", allow_pickle=True)\n",
    "    if show_details:\n",
    "        print(\"++++++++++++++++++++++++++++++\")\n",
    "        print(\"---details of graph dataset---\")\n",
    "        print(\"++++++++++++++++++++++++++++++\")\n",
    "        print(\"dataset name:   \", dataset_name)\n",
    "        print(\"feature shape:  \", feat.shape)\n",
    "        print(\"label shape:    \", label.shape)\n",
    "        print(\"adj shape:      \", adj.shape)\n",
    "        print(\"undirected edge num:   \", int(np.nonzero(adj)[0].shape[0]/2))\n",
    "        print(\"category num:          \", max(label)-min(label)+1)\n",
    "        print(\"category distribution: \")\n",
    "        for i in range(max(label)+1):\n",
    "            print(\"label\", i, end=\":\")\n",
    "            print(len(label[np.where(label == i)]))\n",
    "        print(\"++++++++++++++++++++++++++++++\")\n",
    "    feat_pca = feat[:,:-1]\n",
    "    # X pre-processing\n",
    "    pca = PCA(n_components=19)\n",
    "    feat_pca = pca.fit_transform(feat_pca)\n",
    "    result = np.column_stack((feat_pca, feat[:,-1]))\n",
    "    return result, label, adj\n",
    "\n",
    "\n",
    "def normalize_adj(adj, self_loop=True, symmetry=False):\n",
    "    \"\"\"\n",
    "    normalize the adj matrix\n",
    "    :param adj: input adj matrix\n",
    "    :param self_loop: if add the self loop or not\n",
    "    :param symmetry: symmetry normalize or not\n",
    "    :return: the normalized adj matrix\n",
    "    \"\"\"\n",
    "    # add the self_loop\n",
    "    if self_loop:\n",
    "        adj_tmp = adj + np.eye(adj.shape[0])\n",
    "    else:\n",
    "        adj_tmp = adj\n",
    "\n",
    "    # calculate degree matrix and it's inverse matrix\n",
    "    d = np.diag(adj_tmp.sum(0))\n",
    "    d_inv = np.linalg.inv(d)\n",
    "\n",
    "    # symmetry normalize: D^{-0.5} A D^{-0.5}\n",
    "    if symmetry:\n",
    "        sqrt_d_inv = np.sqrt(d_inv)\n",
    "        norm_adj = np.matmul(np.matmul(sqrt_d_inv, adj_tmp), adj_tmp)\n",
    "\n",
    "    # non-symmetry normalize: D^{-1} A\n",
    "    else:\n",
    "        norm_adj = np.matmul(d_inv, adj_tmp)\n",
    "\n",
    "    return norm_adj\n",
    "\n",
    "\n",
    "def gaussian_noised_feature(X):\n",
    "    \"\"\"\n",
    "    add gaussian noise to the attribute matrix X\n",
    "    Args:\n",
    "        X: the attribute matrix\n",
    "    Returns: the noised attribute matrix X_tilde\n",
    "    \"\"\"\n",
    "    N_1 = torch.Tensor(np.random.normal(1, 0.1, X.shape)).to(device)\n",
    "    N_2 = torch.Tensor(np.random.normal(1, 0.1, X.shape)).to(device)\n",
    "    X_tilde1 = X * N_1\n",
    "    X_tilde2 = X * N_2\n",
    "    return X_tilde1, X_tilde2\n",
    "\n",
    "\n",
    "def diffusion_adj(adj, mode=\"ppr\", transport_rate=0.2):\n",
    "    \"\"\"\n",
    "    graph diffusion\n",
    "    :param adj: input adj matrix\n",
    "    :param mode: the mode of graph diffusion\n",
    "    :param transport_rate: the transport rate\n",
    "    - personalized page rank\n",
    "    -\n",
    "    :return: the graph diffusion\n",
    "    \"\"\"\n",
    "    # add the self_loop\n",
    "    adj_tmp = adj + np.eye(adj.shape[0])\n",
    "\n",
    "    # calculate degree matrix and it's inverse matrix\n",
    "    d = np.diag(adj_tmp.sum(0))\n",
    "    d_inv = np.linalg.inv(d)\n",
    "    sqrt_d_inv = np.sqrt(d_inv)\n",
    "\n",
    "    # calculate norm adj\n",
    "    norm_adj = np.matmul(np.matmul(sqrt_d_inv, adj_tmp), sqrt_d_inv)\n",
    "\n",
    "    # calculate graph diffusion\n",
    "    if mode == \"ppr\":\n",
    "        diff_adj = transport_rate * np.linalg.inv((np.eye(d.shape[0]) - (1 - transport_rate) * norm_adj))\n",
    "\n",
    "    return diff_adj\n",
    "\n",
    "\n",
    "def remove_edge(A, similarity, remove_rate=0.1):\n",
    "    \"\"\"\n",
    "    remove edge based on embedding similarity\n",
    "    Args:\n",
    "        A: the origin adjacency matrix\n",
    "        similarity: cosine similarity matrix of embedding\n",
    "        remove_rate: the rate of removing linkage relation\n",
    "    Returns:\n",
    "        Am: edge-masked adjacency matrix\n",
    "    \"\"\"\n",
    "    # remove edges based on cosine similarity of embedding\n",
    "    n_node = A.shape[0]\n",
    "    for i in range(n_node):\n",
    "        A[i, torch.argsort(similarity[i].cpu())[:int(round(remove_rate * n_node))]] = 0\n",
    "\n",
    "    # normalize adj\n",
    "    Am = normalize_adj(A, self_loop=True, symmetry=False)\n",
    "    Am = numpy_to_torch(Am).to(device)\n",
    "    return Am\n",
    "\n",
    "\n",
    "def load_pretrain_parameter(model):\n",
    "    \"\"\"\n",
    "    load pretrained parameters\n",
    "    Args:\n",
    "        model: Dual Correlation Reduction Network\n",
    "    Returns: model\n",
    "    \"\"\"\n",
    "    pretrained_dict = torch.load('model_pretrain/{}_pretrain.pkl'.format(name), map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_init(model, X, y, A_norm):\n",
    "    \"\"\"\n",
    "    load the pre-train model and calculate similarity and cluster centers\n",
    "    Args:\n",
    "        model: Dual Correlation Reduction Network\n",
    "        X: input feature matrix\n",
    "        y: input label\n",
    "        A_norm: normalized adj\n",
    "    Returns: embedding similarity matrix\n",
    "    \"\"\"\n",
    "    # load pre-train model\n",
    "    # model = load_pretrain_parameter(model)\n",
    "\n",
    "    # calculate embedding similarity\n",
    "    with torch.no_grad():\n",
    "        _, _, _, sim, _, _, _, Z, _, _ = model(X, A_norm, X, A_norm)\n",
    "\n",
    "    # calculate cluster centers\n",
    "    acc, nmi, ari, f1, centers = clustering(Z, y)\n",
    "\n",
    "    return sim, centers\n",
    "\n",
    "\n",
    "# the reconstruction function from DFCN\n",
    "def reconstruction_loss(X, A_norm, X_hat, Z_hat, A_hat):\n",
    "    \"\"\"\n",
    "    reconstruction loss L_{}\n",
    "    Args:\n",
    "        X: the origin feature matrix\n",
    "        A_norm: the normalized adj\n",
    "        X_hat: the reconstructed X\n",
    "        Z_hat: the reconstructed Z\n",
    "        A_hat: the reconstructed A\n",
    "    Returns: the reconstruction loss\n",
    "    \"\"\"\n",
    "    loss_ae = F.mse_loss(X_hat, X)\n",
    "    loss_w = F.mse_loss(Z_hat, torch.spmm(A_norm, X))\n",
    "    loss_a = F.mse_loss(A_hat, A_norm.to_dense())\n",
    "    loss_igae = loss_w + 0.1 * loss_a\n",
    "    loss_rec = loss_ae + loss_igae\n",
    "    return loss_rec\n",
    "\n",
    "\n",
    "def target_distribution(Q):\n",
    "    \"\"\"\n",
    "    calculate the target distribution (student-t distribution)\n",
    "    Args:\n",
    "        Q: the soft assignment distribution\n",
    "    Returns: target distribution P\n",
    "    \"\"\"\n",
    "    weight = Q ** 2 / Q.sum(0)\n",
    "    P = (weight.t() / weight.sum(1)).t()\n",
    "    return P\n",
    "\n",
    "\n",
    "# clustering guidance from DFCN\n",
    "def distribution_loss(Q, P):\n",
    "    \"\"\"\n",
    "    calculate the clustering guidance loss L_{KL}\n",
    "    Args:\n",
    "        Q: the soft assignment distribution\n",
    "        P: the target distribution\n",
    "    Returns: L_{KL}\n",
    "    \"\"\"\n",
    "    loss = F.kl_div((Q[0].log() + Q[1].log() + Q[2].log()) / 3, P, reduction='batchmean')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def r_loss(AZ, Z):\n",
    "    \"\"\"\n",
    "    the loss of propagated regularization (L_R)\n",
    "    Args:\n",
    "        AZ: the propagated embedding\n",
    "        Z: embedding\n",
    "    Returns: L_R\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            p_output = F.softmax(AZ[i][j], dim=1)\n",
    "            q_output = F.softmax(Z[i][j], dim=1)\n",
    "            log_mean_output = ((p_output + q_output) / 2).log()\n",
    "            loss += (F.kl_div(log_mean_output, p_output, reduction='batchmean') +\n",
    "                     F.kl_div(log_mean_output, p_output, reduction='batchmean')) / 2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def off_diagonal(x):\n",
    "    \"\"\"\n",
    "    off-diagonal elements of x\n",
    "    Args:\n",
    "        x: the input matrix\n",
    "    Returns: the off-diagonal elements of x\n",
    "    \"\"\"\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "\n",
    "def cross_correlation(Z_v1, Z_v2):\n",
    "    \"\"\"\n",
    "    calculate the cross-view correlation matrix S\n",
    "    Args:\n",
    "        Z_v1: the first view embedding\n",
    "        Z_v2: the second view embedding\n",
    "    Returns: S\n",
    "    \"\"\"\n",
    "    return torch.mm(F.normalize(Z_v1, dim=1), F.normalize(Z_v2, dim=1).t())\n",
    "\n",
    "\n",
    "def correlation_reduction_loss(S):\n",
    "    \"\"\"\n",
    "    the correlation reduction loss L: MSE for S and I (identical matrix)\n",
    "    Args:\n",
    "        S: the cross-view correlation matrix S\n",
    "    Returns: L\n",
    "    \"\"\"\n",
    "    return torch.diagonal(S).add(-1).pow(2).mean() + off_diagonal(S).pow(2).mean()\n",
    "\n",
    "\n",
    "def dicr_loss(Z_ae, Z_igae, AZ, Z):\n",
    "    \"\"\"\n",
    "    Dual Information Correlation Reduction loss L_{DICR}\n",
    "    Args:\n",
    "        Z_ae: AE embedding including two-view node embedding [0, 1] and two-view cluster-level embedding [2, 3]\n",
    "        Z_igae: IGAE embedding including two-view node embedding [0, 1] and two-view cluster-level embedding [2, 3]\n",
    "        AZ: the propagated fusion embedding AZ\n",
    "        Z: the fusion embedding Z\n",
    "    Returns:\n",
    "        L_{DICR}\n",
    "    \"\"\"\n",
    "    # Sample-level Correlation Reduction (SCR)\n",
    "    # cross-view sample correlation matrix\n",
    "    S_N_ae = cross_correlation(Z_ae[0], Z_ae[1])\n",
    "    S_N_igae = cross_correlation(Z_igae[0], Z_igae[1])\n",
    "    # loss of SCR\n",
    "    L_N_ae = correlation_reduction_loss(S_N_ae)\n",
    "    L_N_igae = correlation_reduction_loss(S_N_igae)\n",
    "\n",
    "    # Feature-level Correlation Reduction (FCR)\n",
    "    # cross-view feature correlation matrix\n",
    "    S_F_ae = cross_correlation(Z_ae[2].t(), Z_ae[3].t())\n",
    "    S_F_igae = cross_correlation(Z_igae[2].t(), Z_igae[3].t())\n",
    "\n",
    "    # loss of FCR\n",
    "    L_F_ae = correlation_reduction_loss(S_F_ae)\n",
    "    L_F_igae = correlation_reduction_loss(S_F_igae)\n",
    "\n",
    "    if name == \"dblp\" or name == \"acm\":\n",
    "        L_N = 0.01 * L_N_ae + 10 * L_N_igae\n",
    "        L_F = 0.5 * L_F_ae + 0.5 * L_F_igae\n",
    "    else:\n",
    "        L_N = 0.1 * L_N_ae + 5 * L_N_igae\n",
    "        L_F = L_F_ae + L_F_igae\n",
    "\n",
    "    # propagated regularization\n",
    "    L_R = r_loss(AZ, Z)\n",
    "\n",
    "    # loss of DICR\n",
    "    loss_dicr = L_N + L_F + gamma_value * L_R\n",
    "\n",
    "    return loss_dicr\n",
    "\n",
    "\n",
    "def clustering(Z, y):\n",
    "    \"\"\"\n",
    "    clustering based on embedding\n",
    "    Args:\n",
    "        Z: the input embedding\n",
    "        y: the ground truth\n",
    "\n",
    "    Returns: acc, nmi, ari, f1, clustering centers\n",
    "    \"\"\"\n",
    "    model = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "    cluster_id = model.fit_predict(Z.data.cpu().numpy())\n",
    "    acc, nmi, ari, f1 = eva(y, cluster_id, show_details=show_training_details)\n",
    "    return acc, nmi, ari, f1, model.cluster_centers_\n",
    "\n",
    "\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate clustering acc and f1-score\n",
    "    Args:\n",
    "        y_true: the ground truth\n",
    "        y_pred: the clustering id\n",
    "\n",
    "    Returns: acc and f1-score\n",
    "    \"\"\"\n",
    "    y_true = y_true - np.min(y_true)\n",
    "    l1 = list(set(y_true))\n",
    "    num_class1 = len(l1)\n",
    "    l2 = list(set(y_pred))\n",
    "    num_class2 = len(l2)\n",
    "    ind = 0\n",
    "    if num_class1 != num_class2:\n",
    "        for i in l1:\n",
    "            if i in l2:\n",
    "                pass\n",
    "            else:\n",
    "                y_pred[ind] = i\n",
    "                ind += 1\n",
    "    l2 = list(set(y_pred))\n",
    "    numclass2 = len(l2)\n",
    "    if num_class1 != numclass2:\n",
    "        print('error')\n",
    "        return\n",
    "    cost = np.zeros((num_class1, numclass2), dtype=int)\n",
    "    for i, c1 in enumerate(l1):\n",
    "        mps = [i1 for i1, e1 in enumerate(y_true) if e1 == c1]\n",
    "        for j, c2 in enumerate(l2):\n",
    "            mps_d = [i1 for i1 in mps if y_pred[i1] == c2]\n",
    "            cost[i][j] = len(mps_d)\n",
    "    m = Munkres()\n",
    "    cost = cost.__neg__().tolist()\n",
    "    indexes = m.compute(cost)\n",
    "    new_predict = np.zeros(len(y_pred))\n",
    "    for i, c in enumerate(l1):\n",
    "        c2 = l2[indexes[i][1]]\n",
    "        ai = [ind for ind, elm in enumerate(y_pred) if elm == c2]\n",
    "        new_predict[ai] = c\n",
    "    acc = metrics.accuracy_score(y_true, new_predict)\n",
    "    f1_macro = metrics.f1_score(y_true, new_predict, average='macro')\n",
    "    return acc, f1_macro\n",
    "\n",
    "\n",
    "def eva(y_true, y_pred, show_details=True):\n",
    "    \"\"\"\n",
    "    evaluate the clustering performance\n",
    "    Args:\n",
    "        y_true: the ground truth\n",
    "        y_pred: the predicted label\n",
    "        show_details: if print the details\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    acc, f1 = cluster_acc(y_true, y_pred)\n",
    "    nmi = nmi_score(y_true, y_pred, average_method='arithmetic')\n",
    "    ari = ari_score(y_true, y_pred)\n",
    "    if show_details:\n",
    "        print(':acc {:.4f}'.format(acc), ', nmi {:.4f}'.format(nmi), ', ari {:.4f}'.format(ari),\n",
    "              ', f1 {:.4f}'.format(f1))\n",
    "    return acc, nmi, ari, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696b95f0-30e9-486e-a389-4091d2b3362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, A, A_norm, Ad):\n",
    "    \"\"\"\n",
    "    train our model\n",
    "    Args:\n",
    "        model: Dual Correlation Reduction Network\n",
    "        X: input feature matrix\n",
    "        y: input label\n",
    "        A: input origin adj\n",
    "        A_norm: normalized adj\n",
    "        Ad: graph diffusion\n",
    "    Returns: acc, nmi, ari, f1\n",
    "    \"\"\"\n",
    "    print(\"Training…\")\n",
    "    # calculate embedding similarity and cluster centers\n",
    "    sim, centers = model_init(model, X, y, A_norm)\n",
    "\n",
    "    # initialize cluster centers\n",
    "    model.cluster_centers.data = torch.tensor(centers).to(device)\n",
    "\n",
    "    # edge-masked adjacency matrix (Am): remove edges based on feature-similarity\n",
    "    Am = remove_edge(A, sim, remove_rate=0.1)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    for epoch in tqdm(range(400)):\n",
    "        # add gaussian noise to X\n",
    "        X_tilde1, X_tilde2 = gaussian_noised_feature(X)\n",
    "\n",
    "        # input & output\n",
    "        X_hat, Z_hat, A_hat, _, Z_ae_all, Z_gae_all, Q, Z, AZ_all, Z_all = model(X_tilde1, Ad, X_tilde2, Am)\n",
    "\n",
    "        # calculate loss: L_{DICR}, L_{REC} and L_{KL}\n",
    "        L_DICR = dicr_loss(Z_ae_all, Z_gae_all, AZ_all, Z_all)\n",
    "        L_REC = reconstruction_loss(X, A_norm, X_hat, Z_hat, A_hat)\n",
    "        L_KL = distribution_loss(Q, target_distribution(Q[0].data))\n",
    "        loss = L_DICR + L_REC + lambda_value * L_KL\n",
    "\n",
    "        # optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # clustering & evaluation\n",
    "        acc, nmi, ari, f1, _ = clustering(Z, y)\n",
    "        if acc > acc:\n",
    "            acc = acc\n",
    "            nmi = nmi\n",
    "            ari = ari\n",
    "            f1 = f1\n",
    "\n",
    "    return acc, nmi, ari, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d78cd26-c589-4c9e-a884-5214bfcb7482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "---details of graph dataset---\n",
      "++++++++++++++++++++++++++++++\n",
      "dataset name:    amac\n",
      "feature shape:   (13752, 768)\n",
      "label shape:     (13752,)\n",
      "adj shape:       (13752, 13752)\n",
      "undirected edge num:    80062\n",
      "category num:           10\n",
      "category distribution: \n",
      "label 0:436\n",
      "label 1:2142\n",
      "label 2:1414\n",
      "label 3:542\n",
      "label 4:5158\n",
      "label 5:308\n",
      "label 6:487\n",
      "label 7:818\n",
      "label 8:2156\n",
      "label 9:291\n",
      "++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# data pre-precessing: X, y, A, A_norm, Ad\n",
    "X, y, A = load_graph_data(name, show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13eaa5a5-0486-4712-8e85-eef41cdcad0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.22496248, -0.05918236, -1.32714092, ..., -0.2527425 ,\n",
       "        -0.24496511,  0.71640363],\n",
       "       [ 7.17497864,  3.51674409, -0.52658413, ..., -0.67454517,\n",
       "        -0.52133049,  1.25078683],\n",
       "       [-3.75418086, -0.87597688,  0.66150691, ..., -0.79053958,\n",
       "         0.52567082,  1.15531224],\n",
       "       ...,\n",
       "       [ 5.21436111, -2.32795365,  0.69547111, ...,  1.16317835,\n",
       "        -0.76829312,  0.83965322],\n",
       "       [-4.53505721,  1.79475444, -1.09565407, ..., -0.2551059 ,\n",
       "         0.02210562,  1.26240483],\n",
       "       [ 0.14297941,  1.59708   ,  1.42124636, ...,  0.12670184,\n",
       "         0.08286483,  0.80681685]], shape=(13752, 20))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d31980-6f46-4ed6-b025-de79559d17b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_norm = normalize_adj(A, self_loop=True, symmetry=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94f4a19f-10fc-46a5-9565-787e65c0c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ad = diffusion_adj(A, mode=\"ppr\", transport_rate=alpha_value)\n",
    "\n",
    "# to torch tensor\n",
    "X = numpy_to_torch(X).to(device)\n",
    "A_norm = numpy_to_torch(A_norm, sparse=True).to(device)\n",
    "Ad = numpy_to_torch(Ad).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98c39c80-abb0-495b-a7ce-c6b9b933ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dual Correlation Reduction Network\n",
    "model = DCRN(n_node=X.shape[0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea0f124d-06b8-49b1-8b88-934ebae2a17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6b5636b82341a68294148b9eb31cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.3421, NMI: 0.2679, ARI: 0.1737, F1: 0.1962\n"
     ]
    }
   ],
   "source": [
    "# deep graph clustering\n",
    "acc, nmi, ari, f1 = train(model, X, y, A, A_norm, Ad)\n",
    "print(\"ACC: {:.4f},\".format(acc), \"NMI: {:.4f},\".format(nmi), \"ARI: {:.4f},\".format(ari), \"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d7e32-cab7-4c42-b6af-5b7e0bef4d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e157dd-8bf2-443d-b6e8-f06947383a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8777919-af98-459b-ae91-36fabfd650d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb281f1492c648549458e30456d736ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.3253, NMI: 0.2003, ARI: 0.0911, F1: 0.2283\n"
     ]
    }
   ],
   "source": [
    "# deep graph clustering\n",
    "acc, nmi, ari, f1 = train(model, X, y, A, A_norm, Ad)\n",
    "print(\"ACC: {:.4f},\".format(acc), \"NMI: {:.4f},\".format(nmi), \"ARI: {:.4f},\".format(ari), \"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a410afe1-fbc3-4716-908c-7a5b6720d47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e0004-5bf7-4267-80a9-953e44f9d651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e12935-8190-4ed2-9c74-5d318640421e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1593cd2e-71f9-4474-bf38-78a06e58b214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0cc06b02b642608e4bbbe41f2cd44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.2256, NMI: 0.0824, ARI: 0.0178, F1: 0.1874\n"
     ]
    }
   ],
   "source": [
    "# deep graph clustering\n",
    "acc, nmi, ari, f1 = train(model, X, y, A, A_norm, Ad)\n",
    "print(\"ACC: {:.4f},\".format(acc), \"NMI: {:.4f},\".format(nmi), \"ARI: {:.4f},\".format(ari), \"F1: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c67fa6-987f-43dd-8ef8-d938acded292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "12.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)   # None means CPU-only build\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96393c1b-69e0-47f1-bbdd-57b964a47a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb85568b-ca9d-4596-aa7f-f3110e7f6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./DCRN-main/dataset/amac/amac_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04674c5e-02cb-4170-abd9-d054fb350da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb76b16-b148-4503-8ef5-c0fad51e8cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 8, ..., 8, 4, 0], shape=(13752,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b57140-1968-47e2-aa44-4f5b567104c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = np.load('./DCRN-main/dataset/amac/amac_feat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4231c7-b199-42a5-81db-67064f3f5745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "068d6f90-afe7-4536-8392-4c5bb237d936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 1., 1., 0.]], shape=(13752, 767), dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0631af-cad3-4c50-b2db-f0bb32baca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_adj = np.load('./DCRN-main/dataset/dblp/dblp_adj.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6775ad-4b10-4743-ac0e-6e7b11b0235b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], shape=(4057, 4057), dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c759f25-fea2-40c8-a841-d7bb11e99600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.000e+00, 1.000e+00, 2.000e+00, 3.000e+00, 4.000e+00, 5.000e+00,\n",
       "       6.000e+00, 7.000e+00, 8.000e+00, 9.000e+00, 1.000e+01, 1.100e+01,\n",
       "       1.200e+01, 1.300e+01, 1.400e+01, 1.500e+01, 1.600e+01, 1.700e+01,\n",
       "       1.800e+01, 1.900e+01, 2.000e+01, 2.100e+01, 2.200e+01, 2.300e+01,\n",
       "       2.400e+01, 2.500e+01, 2.600e+01, 2.700e+01, 2.800e+01, 2.900e+01,\n",
       "       3.000e+01, 3.100e+01, 3.200e+01, 3.300e+01, 3.400e+01, 3.500e+01,\n",
       "       3.600e+01, 3.700e+01, 3.800e+01, 3.900e+01, 4.000e+01, 4.100e+01,\n",
       "       4.200e+01, 4.300e+01, 4.400e+01, 4.500e+01, 4.600e+01, 4.700e+01,\n",
       "       4.800e+01, 4.900e+01, 5.000e+01, 5.100e+01, 5.200e+01, 5.300e+01,\n",
       "       5.400e+01, 5.500e+01, 5.600e+01, 5.700e+01, 5.800e+01, 5.900e+01,\n",
       "       6.000e+01, 6.100e+01, 6.200e+01, 6.300e+01, 6.400e+01, 6.500e+01,\n",
       "       6.600e+01, 6.700e+01, 6.800e+01, 6.900e+01, 7.000e+01, 7.100e+01,\n",
       "       7.200e+01, 7.300e+01, 7.400e+01, 7.500e+01, 7.600e+01, 7.700e+01,\n",
       "       7.800e+01, 7.900e+01, 8.000e+01, 8.100e+01, 8.200e+01, 8.300e+01,\n",
       "       8.400e+01, 8.500e+01, 8.600e+01, 8.700e+01, 8.800e+01, 8.900e+01,\n",
       "       9.000e+01, 9.100e+01, 9.200e+01, 9.300e+01, 9.400e+01, 9.500e+01,\n",
       "       9.600e+01, 9.700e+01, 9.800e+01, 9.900e+01, 1.000e+02, 1.010e+02,\n",
       "       1.020e+02, 1.030e+02, 1.040e+02, 1.050e+02, 1.060e+02, 1.070e+02,\n",
       "       1.080e+02, 1.090e+02, 1.100e+02, 1.110e+02, 1.120e+02, 1.130e+02,\n",
       "       1.140e+02, 1.150e+02, 1.160e+02, 1.170e+02, 1.180e+02, 1.190e+02,\n",
       "       1.200e+02, 1.210e+02, 1.220e+02, 1.230e+02, 1.250e+02, 1.260e+02,\n",
       "       1.270e+02, 1.280e+02, 1.310e+02, 1.350e+02, 1.360e+02, 1.370e+02,\n",
       "       1.390e+02, 1.400e+02, 1.420e+02, 1.430e+02, 1.450e+02, 1.460e+02,\n",
       "       1.470e+02, 1.490e+02, 1.500e+02, 1.510e+02, 1.530e+02, 1.540e+02,\n",
       "       1.570e+02, 1.590e+02, 1.600e+02, 1.610e+02, 1.620e+02, 1.630e+02,\n",
       "       1.650e+02, 1.670e+02, 1.680e+02, 1.700e+02, 1.710e+02, 1.730e+02,\n",
       "       1.750e+02, 1.780e+02, 1.830e+02, 1.840e+02, 1.850e+02, 1.860e+02,\n",
       "       1.880e+02, 1.890e+02, 1.900e+02, 1.930e+02, 1.950e+02, 1.970e+02,\n",
       "       2.000e+02, 2.040e+02, 2.050e+02, 2.070e+02, 2.110e+02, 2.130e+02,\n",
       "       2.170e+02, 2.200e+02, 2.220e+02, 2.270e+02, 2.310e+02, 2.330e+02,\n",
       "       2.410e+02, 2.420e+02, 2.470e+02, 2.480e+02, 2.520e+02, 2.600e+02,\n",
       "       2.620e+02, 2.660e+02, 2.700e+02, 2.750e+02, 2.760e+02, 2.800e+02,\n",
       "       2.810e+02, 2.820e+02, 3.020e+02, 3.070e+02, 3.160e+02, 3.210e+02,\n",
       "       3.230e+02, 3.280e+02, 3.300e+02, 3.390e+02, 3.620e+02, 3.630e+02,\n",
       "       3.650e+02, 3.710e+02, 3.870e+02, 3.960e+02, 4.030e+02, 4.340e+02,\n",
       "       4.420e+02, 4.540e+02, 4.670e+02, 4.740e+02, 5.200e+02, 5.840e+02,\n",
       "       6.350e+02, 7.820e+02, 9.250e+02, 1.247e+03, 1.254e+03, 1.495e+03])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data_adj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kaveh)",
   "language": "python",
   "name": "kaveh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
